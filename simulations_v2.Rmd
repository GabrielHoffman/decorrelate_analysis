
---
title: "Decorrelate simulations"
subtitle: ''
author: "Developed by [Gabriel Hoffman](http://gabrielhoffman.github.io/)"
date: "Run on `r Sys.time()`"
documentclass: article
output: 
  html_document:
  toc: true
  smart: false
vignette: >
  %\VignetteIndexEntry{Decorrelate}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  %\usepackage[utf8]{inputenc}
---


<!--- 

cd /sc/arion/work/hoffmg01/decorrelate_analysis
ml pandoc gcc/11.2.0

R
system("git pull"); rmarkdown::render("simulations_v2.Rmd");


https://hoffmg01.hpc.mssm.edu/decorrelate_analysis/simulations_v2.html

--->



```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  warning=FALSE,
  message=FALSE,
  error = FALSE,
  tidy = FALSE,
  eval = TRUE,
  dev = c("png", "pdf"),
  cache = TRUE)
```

```{r load.packages, cache=FALSE}
suppressPackageStartupMessages({
library(decorrelate)
library(Matrix)
library(Rfast)
library(ggplot2)
library(corpcor)
library(ShrinkCovMat)
library(parallel)
library(cowplot)
library(corpcor)
library(tidyverse)
library(fANCOVA)
library(CovTools)
library(RhpcBLASctl)
})

omp_set_num_threads(6)


# loess = function(formula, data, ...){
#   # fit = loess.as(data$x, data$y, degree=0)
#   fit = loess(y~x, data=data, span=.3)
#   fit
# }

# norm of matrix compared to identity,
# This is the root mean squared error of the off diagonal entries
normCov = function(Sigma){

  if( length(Sigma) == 1)
    if( is.na(Sigma) ) return(NA)
  # base::norm(Sigma - diag(1, nrow(Sigma)), "F")^2 / length(Sigma)

  mse = mean((Sigma-diag(1, nrow(Sigma)))^2)
  sqrt(mse)
}

# matrix square root
msqrt = function(S){
  dcmp = eigen(S)
  # with(dcmp, vectors %*% diag(sqrt(values)) %*% t(vectors))
  with(dcmp, vectors %*% (sqrt(values) * t(vectors)))
}

minvsqrt = function(S){
  dcmp = eigen(S)
  # with(dcmp, vectors %*% diag(1/sqrt(values)) %*% t(vectors))
  with(dcmp, vectors %*% ((1/sqrt(values)) * t(vectors)))
}


# whiten with pseudoinverse with fixed rank
get_w_ginv = function(X, k){
  C = cov(X)
  dcmp = eigen(C)
  # W = with(dcmp, vectors[,seq(k)] %*% diag(1/sqrt(values[seq(k)])) %*% t(vectors[,seq(k)]))
  W = with(dcmp, vectors[,seq(k)] %*% ((1/sqrt(values[seq(k)])) * t(vectors[,seq(k)])))
  W
}


run_simulation = function(n, p_array, sigma_method, rho){

  df = mclapply(p_array, function(p){

    # make sure p is even
    p = 4*as.integer(p/4)

    message(p)

    # create correlation matrix
    Sigma = switch( sigma_method,
        "autocorr" = autocorr.mat(p, rho), 
        "constant" = matrix(rho, p, p), 
        "block" = bdiag(matrix(rho, p/4, p/4), matrix(rho, p/4, p/4), matrix(rho, p/4, p/4), matrix(rho, p/4, p/4)),
        "blockauto" = bdiag(autocorr.mat(p/4, rho), autocorr.mat(p/4, rho), autocorr.mat(p/4, rho), autocorr.mat(p/4, rho)))
    diag(Sigma) = 1

    X_latent = scale(matrnorm(n,p))

    # sample matrix from MVN with covariance Sigma
    Y = X_latent %*% msqrt(Sigma)

    Ys = scale(Y)

    df = data.frame()

    # Eval methods
    #-------------

    # decorrelate
    tm = system.time({
      ecl <- eclairs( Ys[idx_train,])
      y_white <- decorrelate(Ys[-idx_train,], ecl)
      })   
    rmse = normCov(cora(y_white))
    df = rbind(df, data.frame(
                        Method = 'GIW-EB (current work)', 
                        t(c(tm)),
                        rmse = rmse))

    tm = system.time({
      ecl <- eclairs( Ys[idx_train,])
      y_white <- decorrelate(Ys[-idx_train,], ecl, lambda = 0)
      })   
    rmse = normCov(cora(y_white))
    df = rbind(df, data.frame(
                        Method = "lambda = 0",  
                        t(c(tm)),
                        rmse = rmse))

    tm = system.time({
      ecl <- eclairs( Ys[idx_train,])
      y_white <- decorrelate(Ys[-idx_train,], ecl, lambda = 0.01)
      })   
    rmse = normCov(cora(y_white))
    df = rbind(df, data.frame(
                        Method = "lambda = 0.01", 
                        t(c(tm)),
                        rmse = rmse))

    tm = system.time({
      ecl <- eclairs( Ys[idx_train,])
      y_white <- decorrelate(Ys[-idx_train,], ecl, lambda = 1e-4)
      })   
    rmse = normCov(cora(y_white))
    df = rbind(df, data.frame(
                        Method = "lambda = 1e-4",  
                        t(c(tm)),
                        rmse = rmse))

    tm = system.time({
      fit <- CovTools::CovEst.2003LW( Ys[idx_train,] )
      y_white <- Ys[-idx_train,] %*% minvsqrt(fit$S)
      })   
    rmse = normCov(cora(y_white))
    df = rbind(df, data.frame(
                        Method = "Ledoit-Wolf",  
                        t(c(tm)),
                        rmse = rmse))

    tm = system.time({
      fit <- CovTools::CovEst.2010OAS( Ys[idx_train,] )
      y_white <- Ys[-idx_train,] %*% minvsqrt(fit$S)
      })   
    rmse = normCov(cora(y_white))
    df = rbind(df, data.frame(
                        Method = "OAS",  
                        t(c(tm)),
                        rmse = rmse))

    tm = system.time({
      fit <- shrinkcovmat.equal( t(Ys[idx_train,]) )
      y_white <- Ys[-idx_train,] %*% minvsqrt(fit$Sigmahat)
      })   
    rmse = normCov(cora(y_white))
    df = rbind(df, data.frame(
                        Method = "Touloumis", 
                        t(c(tm)),
                        rmse = rmse))

    tm = system.time({
      C <- cor.shrink( Ys[idx_train,], verbose=FALSE )
      y_white <- Ys[-idx_train,] %*% minvsqrt(C)
      })   
    rmse = normCov(cora(y_white))
    df = rbind(df, data.frame(
                        Method = "Sch채fer-Strimmer", 
                        t(c(tm)),
                        rmse = rmse))

    tm = system.time({
      y_white <- Ys[-idx_train,] %*% minvsqrt(Sigma)
      })   
    rmse = normCov(cora(y_white))
    df = rbind(df, data.frame(
                        Method = "Oracle", 
                        t(c(tm)),
                        rmse = rmse))

    tm = system.time({
      # learn transformation
      k <- min(dim(Ys[idx_train,]))-1
      W <- get_w_ginv(Ys[idx_train,], k)
      y_white <- tcrossprod(Ys[-idx_train,], W)
      })   
    rmse = normCov(cora(y_white))
    df = rbind(df, data.frame(
                        Method = "Pseudoinverse", 
                        t(c(tm)),
                        rmse = rmse))    

    df$p = p
    df$rMSE_baseline = normCov(cora(Ys[-idx_train,]))
    df
  }, mc.cores=6)
  do.call(rbind, df)
}
```


# rMSE curve
```{r rMSE}
colMethods = c("GIW-EB (current work)" = "#E41A1C",
              "Ledoit-Wolf" = "#FF7F00", 
              "OAS"= "#cccc29", 
              "Touloumis" = "#A65628", 
              "Sch채fer-Strimmer" = "#f569b3", 
              "Pseudoinverse" = "green3",                
              "lambda = 0" = "navy", 
              "lambda = 0.01" = "#377EB8", 
              "lambda = 1e-4" = "#37b8b2",              
              "Baseline" = "grey50",
              "Oracle" = "black")

df_col = data.frame(Method = factor(names(colMethods),names(colMethods)), 
    color = colMethods, 
    value = 1)

df_col %>% 
  ggplot(aes(x = "", y = value, fill = Method)) + 
  geom_bar(stat = "identity") + 
  coord_polar("y") +
  scale_fill_manual(values = colMethods) +
  theme_void()


set.seed(12)
n = 500 # number of samples
idx_train = 1:(n/2)
p = 300
rho = 0.6

Sigma = bdiag(matrix(rho, p/4, p/4), matrix(rho, p/4, p/4), matrix(rho, p/4, p/4), matrix(rho, p/4, p/4))
    diag(Sigma) = 1

X_latent = scale(matrnorm(n,p))

# sample matrix from MVN with covariance Sigma
Y = X_latent %*% msqrt(Sigma)

# Plot rMSE vs lambda
######################

ecl = eclairs( Y[idx_train,], compute="corr")

df_lambda = c('GIW-EB (current work)' = ecl$lambda,
      "lambda = 0" = 0,
      "lambda = 0.01" = 0.01,
      "lambda = 1e-4" = 1e-4,
      "Ledoit-Wolf" = CovEst.2003LW( Y[idx_train,] )$delta,
      "OAS" = CovEst.2010OAS(Y[idx_train,])$rho,
      "Touloumis" = shrinkcovmat.identity(Y[idx_train,])$lambdahat,
      "Sch채fer-Strimmer" = estimate.lambda(Y[idx_train,], verbose=FALSE) )

df_lambda = data.frame(lambda = df_lambda) %>%
      rownames_to_column %>%
      rename( Method = "rowname")

res = lapply(seq(0.01, .9, length.out=20), function(lambda){
      X_test_white = decorrelate(Y[-idx_train,], ecl, lambda=lambda)
      rMSE = normCov(cora(X_test_white))
      data.frame(lambda = lambda, rMSE = rMSE)
})
res = do.call(rbind, res)

i = which.min(res$rMSE)

ggplot(res, aes(lambda, rMSE)) +
      geom_line(size=1) +
      theme_classic() +
      theme(aspect.ratio=1) +
      xlim(0, 1) +
      geom_vline(data = df_lambda, aes(xintercept=lambda, color=Method), size=2) +
      geom_point(data = res[i,], aes(x = lambda, y = rMSE), color="red") +
      scale_color_manual(values = colMethods) +
      xlab(bquote(lambda)) +
      ylab("Out-of-sample root mean squared error")
```


## Constant correlation: $\rho = 0.6$
```{r run_sim1}
set.seed(12)
n = 1000 # number of samples
idx_train = 1:(n/2)

p_array = as.integer(seq(20, 1.2*n, length.out=300))

# run_simulation(n, p_array, sigma_method, rho)
df = run_simulation(n, p_array, "constant", rho = .6)
df$Method = factor(df$Method, names(colMethods))
```

```{r constant_correlation, cache=FALSE, fig.height=4, fig.width=12}
ymax = df %>%
  filter(((Method == 'lambda = 0') & (p < 0.495*n))) %>%
  summarize(max = max(rmse)) %>%
  pull
  
fig = ggplot(df, aes(p, rmse, color=Method)) + 
  geom_point(alpha=.2) + 
  theme_classic() + 
  theme(aspect.ratio=1, plot.title = element_text(hjust=0.5)) + 
  xlab("Number of features") + 
  ylab("Root Mean Squared Error") + 
  geom_vline(xintercept = n/2, linetype="dashed", color="grey30") +
  coord_fixed(xlim = c(0, NA), ylim=c(0, ymax), expand = FALSE) + 
  scale_color_manual(name="Method",values = colMethods) + 
  geom_smooth(method='loess', se=FALSE, span=.3)

ymax = 1.02*max(df$rmse[df$Method == 'Sch채fer-Strimmer'])
# ymax = .2
ymin = 0.98*min(df$rmse)

plot_grid(fig + theme(legend.position="none"), 
        fig + 
        coord_cartesian(ylim = c(ymin, ymax), expand=FALSE), ncol=2, align="hv", axis="tblr", labels = LETTERS[1:2])
```

```{r relative.error1, cache=FALSE, fig.height=4, fig.width=12}
df2 = df %>%  
  mutate(rel.err = rmse / rMSE_baseline)

ymax = df2 %>%
  filter(((Method == 'lambda = 0') & (p < 0.495*n))) %>%
  summarize(max = max(100*rel.err)) %>%
  pull

fig = df2 %>%
  ggplot(aes(p, rel.err*100, color=Method)) + 
  geom_point(alpha=.2) + 
  theme_classic() + 
  theme(aspect.ratio = 1, plot.title = element_text(hjust=0.5)) + 
  xlab("Number of features") + 
  ylab("Relative reduction in rMSE (%)") +
  geom_vline(xintercept = n/2, linetype="dashed", color="grey30") + 
  coord_fixed(xlim = c(0, NA), ylim=c(0, ymax), expand = FALSE) +
  scale_color_manual(name="Method",values = colMethods) + 
  geom_smooth(method='loess', se=FALSE, span=.35) 

ymax = 1.02*max(100*df2$rel.err[df2$Method == 'Sch채fer-Strimmer'])
# ymax = 30
ymin = .98*min(100*df2$rel.err[df2$Method == 'Oracle'])

plot_grid(fig + theme(legend.position="none"), 
        fig + coord_cartesian(ylim = c(ymin, ymax), expand=FALSE), 
        ncol=2, align="hv", axis="tblr", labels = LETTERS[1:2])
```



```{r time1, fig.height=4, fig.width=6, cache=FALSE}
df %>%
  ggplot(aes(p, user.self, color=Method)) +
  geom_point(alpha=.2) + 
  theme_classic() + 
  theme(aspect.ratio = 1, plot.title = element_text(hjust=0.5)) + 
  xlab("Number of features") + 
  ylab("CPU time (seconds)") +
  scale_color_manual(name="Method",values = colMethods) + 
  geom_smooth(method='loess', se=FALSE) +
  scale_y_log10()
```





## Auto-correlation: $\rho = 0.95$
```{r run_sim2}
set.seed(12)
n = 1000 # number of samples
idx_train = 1:(n/2)
 
p_array = as.integer(seq(20, 1.2*n, length.out=300))

df = run_simulation(n, p_array, "autocorr", rho = .95)
df$Method = factor(df$Method, names(colMethods))
```

```{r auto_correlation, cache=FALSE, fig.height=4, fig.width=12}
ymax = df %>%
  filter(((Method == 'lambda = 0') & (p < 0.495*n))) %>%
  summarize(max = max(rmse)) %>%
  pull
  
fig = ggplot(df, aes(p, rmse, color=Method)) + 
  geom_point(alpha=.2) + 
  theme_classic() + 
  theme(aspect.ratio=1, plot.title = element_text(hjust=0.5)) + 
  xlab("Number of features") + 
  ylab("Root Mean Squared Error") + 
  geom_vline(xintercept = n/2, linetype="dashed", color="grey30") +
  coord_fixed(xlim = c(0, NA), ylim=c(0, ymax), expand = FALSE) + 
  scale_color_manual(name="Method",values = colMethods) + 
  geom_smooth(method='loess', se=FALSE, span=.3)

ymax = 1.02*max(df$rmse[df$Method == 'lambda = 1e-4'])
ymin = 0.98*min(df$rmse)

plot_grid(fig + theme(legend.position="none"), 
        fig + 
        coord_cartesian(ylim = c(ymin, ymax), expand=FALSE), ncol=2, align="hv", axis="tblr", labels = LETTERS[1:2])
```


```{r relative.error2, cache=FALSE, fig.height=4, fig.width=12}
df2 = df %>%  
  mutate(rel.err = rmse / rMSE_baseline)

ymax = df2 %>%
  filter(((Method == 'lambda = 0') & (p < 0.495*n))) %>%
  summarize(max = max(100*rel.err)) %>%
  pull

fig = df2 %>%
  ggplot(aes(p, rel.err*100, color=Method)) + 
  geom_point(alpha=.2) + 
  theme_classic() + 
  theme(aspect.ratio = 1, plot.title = element_text(hjust=0.5)) + 
  xlab("Number of features") + 
  ylab("Relative reduction in rMSE (%)") +
  geom_vline(xintercept = n/2, linetype="dashed", color="grey30") + 
  coord_fixed(xlim = c(0, NA), ylim=c(0, ymax), expand = FALSE) +
  scale_color_manual(name="Method",values = colMethods) + 
  geom_smooth(method='loess', se=FALSE, span=.4) 

ymax = 1.02*max(100*df2$rel.err[df2$Method == 'Sch채fer-Strimmer'])
ymin = .98*min(100*df2$rel.err[df2$Method == 'Oracle'])

plot_grid(fig + theme(legend.position="none"), 
        fig + coord_cartesian(ylim = c(ymin, ymax), expand=FALSE), 
        ncol=2, align="hv", axis="tblr", labels = LETTERS[1:2])
```


```{r time2, fig.height=4, fig.width=6, cache=FALSE}
df %>%
  ggplot(aes(p, user.self, color=Method)) +
  geom_point(alpha=.2) + 
  theme_classic() + 
  theme(aspect.ratio = 1, plot.title = element_text(hjust=0.5)) + 
  xlab("Number of features") + 
  ylab("CPU time (seconds)") +
  scale_color_manual(name="Method",values = colMethods) + 
  geom_smooth(method='loess', se=FALSE) +
  scale_y_log10()
```






## Block correlation: $\rho = 0.6$, 4 blocks
```{r run_sim3}
set.seed(12)
n = 1000 # number of samples
idx_train = 1:(n/2)
  
p_array = as.integer(seq(20, 1.2*n, length.out=300))

df = run_simulation(n, p_array, "block", rho = 0.6)
df$Method = factor(df$Method, names(colMethods))
```

```{r block_correlation, cache=FALSE, fig.height=4, fig.width=12}
ymax = df %>%
  filter(((Method == 'lambda = 0') & (p < 0.495*n))) %>%
  summarize(max = max(rmse)) %>%
  pull
  
fig = ggplot(df, aes(p, rmse, color=Method)) + 
  geom_point(alpha=.2) + 
  theme_classic() + 
  theme(aspect.ratio=1, plot.title = element_text(hjust=0.5)) + 
  xlab("Number of features") + 
  ylab("Root Mean Squared Error") + 
  geom_vline(xintercept = n/2, linetype="dashed", color="grey30") +
  coord_fixed(xlim = c(0, NA), ylim=c(0, ymax), expand = FALSE) + 
  scale_color_manual(name="Method",values = colMethods) + 
  geom_smooth(method='loess', se=FALSE, span=.3)

ymax = 1.02*max(df$rmse[df$Method == 'lambda = 0.01'])
ymin = 0.98*min(df$rmse)

plot_grid(fig + theme(legend.position="none"), 
        fig + 
        coord_cartesian(ylim = c(ymin, ymax), expand=FALSE), ncol=2, align="hv", axis="tblr", labels = LETTERS[1:2])
```


```{r relative.error3, cache=FALSE, fig.height=4, fig.width=12}
df2 = df %>%  
  mutate(rel.err = rmse / rMSE_baseline)

ymax = df2 %>%
  filter(((Method == 'lambda = 0') & (p < 0.495*n))) %>%
  summarize(max = max(100*rel.err)) %>%
  pull

fig = df2 %>%
  ggplot(aes(p, rel.err*100, color=Method)) + 
  geom_point(alpha=.2) + 
  theme_classic() + 
  theme(aspect.ratio = 1, plot.title = element_text(hjust=0.5)) + 
  xlab("Number of features") + 
  ylab("Relative reduction in rMSE (%)") +
  geom_vline(xintercept = n/2, linetype="dashed", color="grey30") + 
  coord_fixed(xlim = c(0, NA), ylim=c(0, ymax), expand = FALSE) +
  scale_color_manual(name="Method",values = colMethods) + 
  geom_smooth(method='loess', se=FALSE, span=.35) 

ymax = 1.02*max(100*df2$rel.err[df2$Method == 'lambda = 0.01'])
ymin = .98*min(100*df2$rel.err[df2$Method == 'Oracle'])

plot_grid(fig + theme(legend.position="none"), 
        fig + coord_cartesian(ylim = c(ymin, ymax), expand=FALSE), 
        ncol=2, align="hv", axis="tblr", labels = LETTERS[1:2])
```


```{r time3, fig.height=4, fig.width=6, cache=FALSE}
df %>%
  ggplot(aes(p, user.self, color=Method)) +
  geom_point(alpha=.2) + 
  theme_classic() + 
  theme(aspect.ratio = 1, plot.title = element_text(hjust=0.5)) + 
  xlab("Number of features") + 
  ylab("CPU time (seconds)") +
  scale_color_manual(name="Method",values = colMethods) + 
  geom_smooth(method='loess', se=FALSE) +
  scale_y_log10()
```




## Block auto-correlation: $\rho = 0.95$, 4 blocks
```{r run_sim4}
set.seed(12)
n = 1000 # number of samples
idx_train = 1:(n/2)
  
p_array = as.integer(seq(20, 1.2*n, length.out=300))

df = run_simulation(n, p_array, "blockauto", rho = 0.95)
df$Method = factor(df$Method, names(colMethods))
df$Method = droplevels(df$Method )
```

```{r block_autocor, cache=FALSE, fig.height=4, fig.width=12}
ymax = df %>%
  filter(((Method == 'lambda = 0') & (p < 0.495*n))) %>%
  summarize(max = max(rmse)) %>%
  pull
  
fig = ggplot(df, aes(p, rmse, color=Method)) + 
  geom_point(alpha=.2) + 
  theme_classic() + 
  theme(aspect.ratio=1, plot.title = element_text(hjust=0.5)) + 
  xlab("Number of features") + 
  ylab("Root Mean Squared Error") + 
  geom_vline(xintercept = n/2, linetype="dashed", color="grey30") +
  coord_fixed(xlim = c(0, NA), ylim=c(0, ymax), expand = FALSE) + 
  scale_color_manual(name="Method",values = colMethods) + 
  geom_smooth(method='loess', se=FALSE, span=.3)

ymax = 1.02*max(df$rmse[df$Method == 'lambda = 1e-4'])
ymin = 0.98*min(df$rmse)

plot_grid(fig + theme(legend.position="none"), 
        fig + 
        coord_cartesian(ylim = c(ymin, ymax), expand=FALSE), ncol=2, align="hv", axis="tblr", labels = LETTERS[1:2])
```


```{r relative.error4, cache=FALSE, fig.height=4, fig.width=12}
df2 = df %>%  
  mutate(rel.err = rmse / rMSE_baseline)

ymax = df2 %>%
  filter(((Method == 'lambda = 0') & (p < 0.495*n))) %>%
  summarize(max = max(100*rel.err)) %>%
  pull

fig = df2 %>%
  ggplot(aes(p, rel.err*100, color=Method)) + 
  geom_point(alpha=.2) + 
  theme_classic() + 
  theme(aspect.ratio = 1, plot.title = element_text(hjust=0.5)) + 
  xlab("Number of features") + 
  ylab("Relative reduction in rMSE (%)") +
  geom_vline(xintercept = n/2, linetype="dashed", color="grey30") + 
  coord_fixed(xlim = c(0, NA), ylim=c(0, ymax), expand = FALSE) +
  scale_color_manual(name="Method",values = colMethods) + 
  geom_smooth(method='loess', se=FALSE, span=.4) 

ymax = 1.02*max(100*df2$rel.err[df2$Method == 'lambda = 0.01'])
ymin = .98*min(100*df2$rel.err[df2$Method == 'Oracle'])

plot_grid(fig + theme(legend.position="none"), 
        fig + coord_cartesian(ylim = c(ymin, ymax), expand=FALSE), 
        ncol=2, align="hv", axis="tblr", labels = LETTERS[1:2])
```


```{r time4, fig.height=4, fig.width=6, cache=FALSE}
df %>%
  ggplot(aes(p, user.self, color=Method)) +
  geom_point(alpha=.2) + 
  theme_classic() + 
  theme(aspect.ratio = 1, plot.title = element_text(hjust=0.5)) + 
  xlab("Number of features") + 
  ylab("CPU time (seconds)") +
  scale_color_manual(name="Method",values = colMethods) + 
  geom_smooth(method='loess', se=FALSE) +
  scale_y_log10()
```




