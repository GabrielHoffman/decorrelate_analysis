
---
title: "Decorrelate simulations"
subtitle: ''
author: "Developed by [Gabriel Hoffman](http://gabrielhoffman.github.io/)"
date: "Run on `r Sys.time()`"
documentclass: article
output: 
  html_document:
  toc: true
  smart: false
vignette: >
  %\VignetteIndexEntry{Decorrelate}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  %\usepackage[utf8]{inputenc}
---


<!--- 

cd /sc/arion/work/hoffmg01/decorrelate_analysis
ml pandoc gcc/11.2.0

R
system("git pull"); rmarkdown::render("simulations_v2.Rmd");


https://hoffmg01.hpc.mssm.edu/decorrelate_analysis/simulations_v2.html

--->

# Touloumis: shrinkcovmat.identity()
# LW: CovTools::CovEst.2003LW()
# OAS: CovTools::CovEst.2010OAS()
# Sch채fer-Strimmer: cor.shrink()
# lambda = 0
# lambda = 1e-4
# lambda = 1e-1
# decorrelate
# pseudo-inverse



```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  warning=FALSE,
  message=FALSE,
  error = FALSE,
  tidy = FALSE,
  eval = TRUE,
  dev = c("png", "pdf"),
  cache = TRUE)
```

```{r load.packages, cache=FALSE}
suppressPackageStartupMessages({
library(decorrelate)
library(Matrix)
library(Rfast)
library(ggplot2)
library(corpcor)
library(ShrinkCovMat)
library(cowplot)
library(corpcor)
library(tidyverse)
library(CovTools)
library(RhpcBLASctl)
})

omp_set_num_threads(6)

# norm of matrix compared to identity,
# This is the root mean squared error of the off diagonal entries
normCov = function(Sigma){

  if( length(Sigma) == 1)
    if( is.na(Sigma) ) return(NA)
  # base::norm(Sigma - diag(1, nrow(Sigma)), "F")^2 / length(Sigma)

  mse = mean((Sigma-diag(1, nrow(Sigma)))^2)
  sqrt(mse)
}

# matrix square root
msqrt = function(S){
  dcmp = eigen(S)
  # with(dcmp, vectors %*% diag(sqrt(values)) %*% t(vectors))
  with(dcmp, vectors %*% (sqrt(values) * t(vectors)))
}

minvsqrt = function(S){
  dcmp = eigen(S)
  # with(dcmp, vectors %*% diag(1/sqrt(values)) %*% t(vectors))
  with(dcmp, vectors %*% ((1/sqrt(values)) * t(vectors)))
}


# whiten with pseudoinverse with fixed rank
get_w_ginv = function(X, k){
  C = cov(X)
  dcmp = eigen(C)
  # W = with(dcmp, vectors[,seq(k)] %*% diag(1/sqrt(values[seq(k)])) %*% t(vectors[,seq(k)]))
  W = with(dcmp, vectors[,seq(k)] %*% ((1/sqrt(values[seq(k)])) * t(vectors[,seq(k)])))
  W
}


run_simulation = function(n, epsilon, p_array, sigma_method, rho){

  df = lapply(p_array, function(p){

    # make sure p is even
    p = 4*as.integer(p/4)

    message(p)

    # create correlation matrix
    Sigma = switch( sigma_method,
        "autocorr" = autocorr.mat(p, rho), 
        "constant" = matrix(rho, p, p), 
        "block" = bdiag(matrix(rho, p/4, p/4), matrix(rho, p/4, p/4), matrix(rho, p/4, p/4), matrix(rho, p/4, p/4)),
        "blockauto" = bdiag(autocorr.mat(p/4, rho), autocorr.mat(p/4, rho), autocorr.mat(p/4, rho), autocorr.mat(p/4, rho)))
    diag(Sigma) = 1

    X_latent = scale(matrnorm(n,p))

    # sample matrix from MVN with covariance Sigma
    Y = X_latent %*% msqrt(Sigma)

    df = data.frame()

    # Eval methods
    #-------------

    # decorrelate
    tm = system.time({
      ecl <- eclairs( Y[idx_train,])
      y_white <- decorrelate(Y[-idx_train,], ecl)
      })   
    rmse = normCov(cora(y_white))
    df = rbind(df, data.frame(
                        Method = 'GIW-EB (current work)', 
                        t(c(tm)),
                        rmse = rmse))

    tm = system.time({
      ecl <- eclairs( Y[idx_train,])
      y_white <- decorrelate(Y[-idx_train,], ecl, lambda = 0)
      })   
    rmse = normCov(cora(y_white))
    df = rbind(df, data.frame(
                        Method = "lambda = 0",  
                        t(c(tm)),
                        rmse = rmse))

    tm = system.time({
      ecl <- eclairs( Y[idx_train,])
      y_white <- decorrelate(Y[-idx_train,], ecl, lambda = 1e-1)
      })   
    rmse = normCov(cora(y_white))
    df = rbind(df, data.frame(
                        Method = "lambda = 0.1", 
                        t(c(tm)),
                        rmse = rmse))

    tm = system.time({
      ecl <- eclairs( Y[idx_train,])
      y_white <- decorrelate(Y[-idx_train,], ecl, lambda = 1e-4)
      })   
    rmse = normCov(cora(y_white))
    df = rbind(df, data.frame(
                        Method = "lambda = 1e-4",  
                        t(c(tm)),
                        rmse = rmse))

    tm = system.time({
      fit <- CovTools::CovEst.2003LW( scale(Y[idx_train,]) )
      y_white <- Y[-idx_train,] %*% minvsqrt(fit$S)
      })   
    rmse = normCov(cora(y_white))
    df = rbind(df, data.frame(
                        Method = "Ledoit-Wolf",  
                        t(c(tm)),
                        rmse = rmse))

    tm = system.time({
      fit <- CovTools::CovEst.2010OAS( scale(Y[idx_train,]) )
      y_white <- Y[-idx_train,] %*% minvsqrt(fit$S)
      })   
    rmse = normCov(cora(y_white))
    df = rbind(df, data.frame(
                        Method = "OAS",  
                        t(c(tm)),
                        rmse = rmse))

    tm = system.time({
      fit <- shrinkcovmat.identity( scale(t(Y[idx_train,])) )
      y_white <- Y[-idx_train,] %*% minvsqrt(fit$Sigmahat)
      })   
    rmse = normCov(cora(y_white))
    df = rbind(df, data.frame(
                        Method = "Touloumis", 
                        t(c(tm)),
                        rmse = rmse))

    tm = system.time({
      C <- cor.shrink( scale(Y[idx_train,]), verbose=FALSE )
      y_white <- Y[-idx_train,] %*% minvsqrt(C)
      })   
    rmse = normCov(cora(y_white))
    df = rbind(df, data.frame(
                        Method = "Sch채fer-Strimmer", 
                        t(c(tm)),
                        rmse = rmse))

    tm = system.time({
      y_white <- Y[-idx_train,] %*% minvsqrt(Sigma)
      })   
    rmse = normCov(cora(y_white))
    df = rbind(df, data.frame(
                        Method = "Oracle", 
                        t(c(tm)),
                        rmse = rmse))

    tm = system.time({
      # learn transformation
      k <- min(dim(Y[idx_train,]))-1
      W <- get_w_ginv(scale(Y[idx_train,]), k)
      y_white <- tcrossprod(Y[-idx_train,], W)
      })   
    rmse = normCov(cora(y_white))
    df = rbind(df, data.frame(
                        Method = "Pseudoinverse", 
                        t(c(tm)),
                        rmse = rmse))    

    tm = system.time({
      # no transformation
      y_white <- Y[-idx_train,]
      })   
    rmse = normCov(cora(y_white))
    df = rbind(df, data.frame(
                        Method = "Baseline", 
                        t(c(tm)),
                        rmse = rmse))

    df$p = p
    df
  })
  do.call(rbind, df)
}
```


# make sure rMSE change formula is same for sims and 1KG
# Want: % decrease in rMSE


# rMSE curve
```{r rMSE}
set.seed(12)
n = 500 # number of samples
idx_train = 1:(n/2)
p = 300
rho = 0.6

Sigma = bdiag(matrix(rho, p/4, p/4), matrix(rho, p/4, p/4), matrix(rho, p/4, p/4), matrix(rho, p/4, p/4))
    diag(Sigma) = 1

X_latent = scale(matrnorm(n,p))

# sample matrix from MVN with covariance Sigma
Y = X_latent %*% msqrt(Sigma)

# Plot rMSE vs lambda
######################
library(tidyverse)

ecl = eclairs( Y[idx_train,], compute="corr")

df_lambda = c('GIW-EB (current work)' = ecl$lambda,
      "0" = 0,
      "0.01" = 0.01,
      "Ledoit-Wolf" = CovEst.2003LW( Y[idx_train,] )$delta,
      "OAS" = CovEst.2010OAS(Y[idx_train,])$rho,
      "Touloumis" = shrinkcovmat.identity(Y[idx_train,])$lambdahat,
      "Sch채fer-Strimmer" = estimate.lambda(Y[idx_train,], verbose=FALSE) )

df_lambda = data.frame(lambda = df_lambda) %>%
      rownames_to_column %>%
      rename( Method = "rowname")

res = lapply(seq(0.01, .9, length.out=20), function(lambda){
      cat("\r", lambda, '   ')
      X_test_white = decorrelate(Y[-idx_train,], ecl, lambda=lambda)
      rMSE = normCov(cora(X_test_white))
      data.frame(lambda = lambda, rMSE = rMSE)
})
res = do.call(rbind, res)

i = which.min(res$rMSE)

ggplot(res, aes(lambda, rMSE)) +
      geom_line(size=1) +
      theme_classic() +
      theme(aspect.ratio=1) +
      xlim(0, 1) +
      geom_vline(data = df_lambda, aes(xintercept=lambda, color=Method), size=2) +
      geom_point(data = res[i,], aes(x = lambda, y = rMSE), color="red")
```


## Constant correlation: $\rho = 0.6$
```{r run_sim1}
set.seed(12)
n = 1000 # number of samples
idx_train = 1:(n/2)
epsilon = 1e-4 

p_array = as.integer(seq(20, 1.2*n, length.out=300))

# run_simulation(n, epsilon, p_array, sigma_method, rho)
df_melt = run_simulation(n, epsilon, p_array, "constant", rho = .6)
```

```{r constant_correlation, cache=FALSE, fig.height=4, fig.width=12}
df_melt$line = "1"
df_melt$line[df_melt$variable == 'Ledoit-Wolf'] = "2"

cols = c(RColorBrewer::brewer.pal(length(unique(df_melt$variable))-1, "Set1"), "black")

cols[cols == "#FFFF33"]= "#cccc29"

fig = ggplot(df_melt, aes(p, value, color=variable, linetype=line)) + 
  geom_point(alpha=.2) + 
  theme_classic() + 
  theme(aspect.ratio=1, plot.title = element_text(hjust=0.5)) + 
  xlab("Number of features") + ylab("Root Mean Squared Error") + 
  geom_vline(xintercept = n/2, linetype="dashed", color="grey30") + 
  scale_x_continuous(limits=c(0, NA), expand=c(0,0)) + 
  scale_y_continuous(limits=c(0, NA), expand=c(0,0)) + 
  scale_color_manual(name="Method",values = cols) + 
  geom_smooth(method='loess', span=.2, se=FALSE) + 
  scale_linetype_manual(values = c("solid", "dashed"))

# ymax = 1.02*max(df_melt$value[df_melt$variable == 'lambda = 0.01'])
ymax = 1.02*max(df_melt$value[df_melt$variable == 'Sch채fer-Strimmer'])
ymin = 0.98*min(df_melt$value)

plot_grid(fig + theme(legend.position="none"), fig + coord_cartesian(ylim = c(ymin, ymax)), ncol=2, align="hv", axis="tblr", labels = LETTERS[1:2])
```

```{r relative.error1, cache=FALSE, fig.height=4, fig.width=12}
df2 = df_melt %>%  
  mutate(rel.err = value / rMSE_baseline)

fig = df2 %>%
  ggplot(aes(p, rel.err*100, color=variable)) + 
  geom_point(alpha=.2) + 
  theme_classic() + 
  theme(aspect.ratio=1, plot.title = element_text(hjust=0.5)) + 
  xlab("Number of features") + 
  ylab("Relative reduction in rMSE (%)") +
  geom_vline(xintercept = n/2, linetype="dashed", color="grey30") + 
  scale_x_continuous(limits=c(0, NA), expand=c(0,0)) + 
  scale_y_continuous(limits=c(0, NA), expand=c(0,0)) + 
  scale_color_manual(name="Method",values = cols) + 
  geom_smooth(method='loess', span=.2, se=FALSE) + 
  scale_linetype_manual(values = c("solid", "dashed"))

ymax = 1.02*max(100*df2$rel.err[df2$variable == 'Sch채fer-Strimmer'])
ymin = .98*min(100*df2$rel.err[df2$variable == 'Oracle'])

plot_grid(fig + theme(legend.position="none"), fig + coord_cartesian(ylim = c(ymin, ymax)), ncol=2, align="hv", axis="tblr", labels = LETTERS[1:2])
```




## Auto-correlation: $\rho = 0.95$
```{r run_sim2}
set.seed(12)
n = 1000 # number of samples
idx_train = 1:(n/2)
epsilon = 1e-4
 
p_array = as.integer(seq(20, 1.2*n, length.out=300))

# run_simulation(n, epsilon, p_array, sigma_method, rho)
df_melt = run_simulation(n, epsilon, p_array, "autocorr", rho = .95)
```

```{r auto_correlation, cache=FALSE, fig.height=4, fig.width=12}
df_melt$line = "1"
df_melt$line[df_melt$variable == 'Ledoit-Wolf'] = "2"

cols = c(RColorBrewer::brewer.pal(length(unique(df_melt$variable))-1, "Set1"), "black")

cols[cols == "#FFFF33"]= "#cccc29"

fig = ggplot(df_melt, aes(p, value, color=variable, linetype=line)) + geom_point(alpha=.2) + theme_classic() + theme(aspect.ratio=1, plot.title = element_text(hjust=0.5)) + xlab("Number of features") + ylab("Root Mean Squared Error") + geom_vline(xintercept = n/2, linetype="dashed", color="grey30") + scale_x_continuous(limits=c(0, NA), expand=c(0,0)) + scale_y_continuous(limits=c(0, NA), expand=c(0,0)) + scale_color_manual(name="Method",values = cols) + geom_smooth(method='loess', span=.2, se=FALSE) + scale_linetype_manual(values = c("solid", "dashed"))

ymax = 1.02*max(df_melt$value[df_melt$variable == 'lambda = 0.01'])
ymin = 0.98*min(df_melt$value)

plot_grid(fig + theme(legend.position="none"), fig + coord_cartesian(ylim = c(ymin, ymax)), ncol=2, align="hv", axis="tblr", labels = LETTERS[1:2])
```


```{r relative.error2, cache=FALSE, fig.height=4, fig.width=12}
df2 = df_melt %>%  
  mutate(rel.err = value / rMSE_baseline)

fig = df2 %>%
  ggplot(aes(p, rel.err*100, color=variable)) + 
  geom_point(alpha=.2) + 
  theme_classic() + 
  theme(aspect.ratio=1, plot.title = element_text(hjust=0.5)) + 
  xlab("Number of features") + 
  ylab("Relative reduction in rMSE (%)") +
  geom_vline(xintercept = n/2, linetype="dashed", color="grey30") + 
  scale_x_continuous(limits=c(0, NA), expand=c(0,0)) + 
  scale_y_continuous(limits=c(0, NA), expand=c(0,0)) + 
  scale_color_manual(name="Method",values = cols) + 
  geom_smooth(method='loess', span=.2, se=FALSE) + 
  scale_linetype_manual(values = c("solid", "dashed"))

ymax = 1.02*max(100*df2$rel.err[df2$variable == 'Sch채fer-Strimmer'])
ymin = .98*min(100*df2$rel.err[df2$variable == 'Oracle'])

plot_grid(fig + theme(legend.position="none"), fig + coord_cartesian(ylim = c(ymin, ymax)), ncol=2, align="hv", axis="tblr", labels = LETTERS[1:2])
```




## Block correlation: $\rho = 0.6$, 4 blocks
```{r run_sim3}
set.seed(12)
n = 1000 # number of samples
idx_train = 1:(n/2)
epsilon = 1e-4
  
p_array = as.integer(seq(20, 1.2*n, length.out=300))

# run_simulation(n, epsilon, p_array, sigma_method, rho)
df_melt = run_simulation(n, epsilon, p_array, "block", rho = 0.6)
```

```{r block_correlation, cache=FALSE, fig.height=4, fig.width=12}
df_melt$line = "1"
df_melt$line[df_melt$variable == 'Ledoit-Wolf'] = "2"
 
cols = c(RColorBrewer::brewer.pal(length(unique(df_melt$variable))-1, "Set1"), "black")

cols[cols == "#FFFF33"]= "#cccc29"

fig = ggplot(df_melt, aes(p, value, color=variable, linetype=line)) + geom_point(alpha=.2) + theme_classic() + theme(aspect.ratio=1, plot.title = element_text(hjust=0.5)) + xlab("Number of features") + ylab("Root Mean Squared Error") + geom_vline(xintercept = n/2, linetype="dashed", color="grey30") + scale_x_continuous(limits=c(0, NA), expand=c(0,0)) + scale_y_continuous(limits=c(0, .35), expand=c(0,0)) + scale_color_manual(name="Method",values = cols) + geom_smooth(method='loess', span=.1, se=FALSE) + scale_linetype_manual(values = c("solid", "dashed"))

ymax = 1.02*max(df_melt$value[df_melt$variable == 'lambda = 0.01'])
ymin = 0.98*min(df_melt$value)

plot_grid(fig + theme(legend.position="none"), fig + coord_cartesian(ylim = c(ymin, ymax)), ncol=2, align="hv", axis="tblr", labels = LETTERS[1:2])
```


```{r relative.error3, cache=FALSE, fig.height=4, fig.width=12}
df2 = df_melt %>%  
  mutate(rel.err = value / rMSE_baseline)

fig = df2 %>%
  ggplot(aes(p, rel.err*100, color=variable)) + 
  geom_point(alpha=.2) + 
  theme_classic() + 
  theme(aspect.ratio=1, plot.title = element_text(hjust=0.5)) + 
  xlab("Number of features") + 
  ylab("Relative reduction in rMSE (%)") +
  geom_vline(xintercept = n/2, linetype="dashed", color="grey30") + 
  scale_x_continuous(limits=c(0, NA), expand=c(0,0)) + 
  scale_y_continuous(limits=c(0, NA), expand=c(0,0)) + 
  scale_color_manual(name="Method",values = cols) + 
  geom_smooth(method='loess', span=.2, se=FALSE) + 
  scale_linetype_manual(values = c("solid", "dashed"))

ymax = 1.02*max(100*df2$rel.err[df2$variable == 'lambda = 0.01'])
ymin = .98*min(100*df2$rel.err[df2$variable == 'Oracle'])

plot_grid(fig + theme(legend.position="none"), fig + coord_cartesian(ylim = c(ymin, ymax)), ncol=2, align="hv", axis="tblr", labels = LETTERS[1:2])
```


## Block auto-correlation: $\rho = 0.95$, 4 blocks
```{r run_sim4}
set.seed(12)
n = 1000 # number of samples
idx_train = 1:(n/2)
epsilon = 1e-4
  
p_array = as.integer(seq(20, 1.2*n, length.out=300))

# run_simulation(n, epsilon, p_array, sigma_method, rho)
df_melt = run_simulation(n, epsilon, p_array, "blockauto", rho = 0.95)
```

```{r block_autocor, cache=FALSE, fig.height=4, fig.width=12}
df_melt$line = "1"
df_melt$line[df_melt$variable == 'Ledoit-Wolf'] = "2"
 
cols = c(RColorBrewer::brewer.pal(length(unique(df_melt$variable))-1, "Set1"), "black")

cols[cols == "#FFFF33"]= "#cccc29"

fig = ggplot(df_melt, aes(p, value, color=variable, linetype=line)) + geom_point(alpha=.2) + theme_classic() + theme(aspect.ratio=1, plot.title = element_text(hjust=0.5)) + xlab("Number of features") + ylab("Root Mean Squared Error") + geom_vline(xintercept = n/2, linetype="dashed", color="grey30") + scale_x_continuous(limits=c(0, NA), expand=c(0,0)) + scale_y_continuous(limits=c(0, .42), expand=c(0,0)) + scale_color_manual(name="Method",values = cols) + geom_smooth(method='loess', span=.2, se=FALSE) + scale_linetype_manual(values = c("solid", "dashed"))

ymax = 1.02*max(df_melt$value[df_melt$variable == 'lambda = 0.01'])
ymin = 0.98*min(df_melt$value)

plot_grid(fig + theme(legend.position="none"), fig + coord_cartesian(ylim = c(ymin, ymax)), ncol=2, align="hv", axis="tblr", labels = LETTERS[1:2])
```


```{r relative.error4, cache=FALSE, fig.height=4, fig.width=12}
df2 = df_melt %>%  
  mutate(rel.err = value / rMSE_baseline)

fig = df2 %>%
  ggplot(aes(p, rel.err*100, color=variable)) + 
  geom_point(alpha=.2) + 
  theme_classic() + 
  theme(aspect.ratio=1, plot.title = element_text(hjust=0.5)) + 
  xlab("Number of features") + 
  ylab("Relative reduction in rMSE (%)") +
  geom_vline(xintercept = n/2, linetype="dashed", color="grey30") + 
  scale_x_continuous(limits=c(0, NA), expand=c(0,0)) + 
  scale_y_continuous(limits=c(0, NA), expand=c(0,0)) + 
  scale_color_manual(name="Method",values = cols) + 
  geom_smooth(method='loess', span=.2, se=FALSE) + 
  scale_linetype_manual(values = c("solid", "dashed"))

ymax = 1.02*max(100*df2$rel.err[df2$variable == 'Sch채fer-Strimmer'])
ymin = .98*min(100*df2$rel.err[df2$variable == 'Oracle'])

plot_grid(fig + theme(legend.position="none"), fig + coord_cartesian(ylim = c(ymin, ymax)), ncol=2, align="hv", axis="tblr", labels = LETTERS[1:2])
```















